import streamlit as st
import pandas as pd
import numpy as np
import xgboost as xgb
import json
import os
import time
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve
import shap


st.markdown("""
<style>
/* –û–±—â–∏–π —Å—Ç–∏–ª—å —Å–∞–π–¥–±–∞—Ä–∞ */
section[data-testid="stSidebar"] {
    background-color: #2D5084 !important;
    color: white !important;
    padding-top: 10px !important;
}

/* –ù–∞–¥–ø–∏—Å–∏ –∏ —Ç–µ–∫—Å—Ç—ã –≤ —Å–∞–π–¥–±–∞—Ä–µ */
section[data-testid="stSidebar"] label,
section[data-testid="stSidebar"] div,
section[data-testid="stSidebar"] span {
    color: white !important;
    font-weight: 500 !important;
    font-size: 16px !important;
}

/* –°–µ–ª–µ–∫—Ç–±–æ–∫—Å –∏ –ø–æ–ª—è –≤–≤–æ–¥–∞ */
section[data-testid="stSidebar"] div[role="combobox"],
section[data-testid="stSidebar"] input,
section[data-testid="stSidebar"] select {
    background-color: #001E48 !important;
    border: none !important;
    border-radius: 8px !important;
    color: white !important;
    box-shadow: none !important;
}

/* SVG –∏–∫–æ–Ω–∫–∏ –≤ —Å–∞–π–¥–±–∞—Ä–µ */
section[data-testid="stSidebar"] svg {
    color: white !important;
}

/* –í—ã–ø–∞–¥–∞—é—â–∏–µ —Å–ø–∏—Å–∫–∏ –∏ –æ–ø—Ü–∏–∏ */
ul[role="listbox"],
li[role="option"] {
    background-color: #001E48 !important;
    color: white !important;
    border: none !important;
}

li[role="option"]:hover,
li[aria-selected="true"] {
    background-color: #003366 !important;
}

/* –£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –≥—Ä–∞–Ω–∏—Ü –∏ —Ç–µ–Ω–µ–π –æ—Ç BaseWeb */
[data-baseweb="select"],
[data-baseweb="select"] * {
    background-color: #001E48 !important;
    color: white !important;
    border: none !important;
    box-shadow: none !important;
}

</style>
""", unsafe_allow_html=True)





# –ü—É—Ç–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
MODEL_PATH_XGB = "model_xgb.json"
MODEL_PATH_LR = "model_lr.json"
LABEL_ENCODERS_PATH = "label_encoders.json"
FEATURES_PATH = "features.json"
SCALER_PATH = "scaler.json"

# –í—ã–±–æ—Ä –≤–∫–ª–∞–¥–∫–∏ —á–µ—Ä–µ–∑ selectbox
menu = st.sidebar.selectbox(
    "–í—ã–±–µ—Ä–∏—Ç–µ –≤–∫–ª–∞–¥–∫—É",
    [
        "üèãÔ∏è‚Äç‚ôÇÔ∏è –û–±—É—á–µ–Ω–∏–µ XGBoost",
        "üìä –û–±—É—á–µ–Ω–∏–µ Logistic Regression",
        "üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ XGBoost",
        "üìä –ü—Ä–æ–≤–µ—Ä–∫–∞ Logistic Regression"
    ]
)

# –í—ã–±–æ—Ä —Ç–∏–ø–∞ –º–æ–¥–µ–ª–∏
if "XGBoost" in menu:
    model_type = "XGBoost" if "–û–±—É—á–µ–Ω–∏–µ" in menu else "xgb"
elif "Logistic Regression" in menu:
    model_type = "Logistic Regression" if "–û–±—É—á–µ–Ω–∏–µ" in menu else "lr"

# –§—É–Ω–∫—Ü–∏–∏

def save_model(model, feature_names, model_path):
    if isinstance(model, xgb.XGBClassifier):
        model.save_model(model_path)
    else:
        with open(model_path, "w") as f:
            json.dump({"coef": model.coef_.tolist(), "intercept": model.intercept_.tolist(), "features": feature_names}, f)
    with open(FEATURES_PATH, "w") as f:
        json.dump(feature_names, f)

def load_model(model_path, model_type):
    if model_type == "xgb":
        model = xgb.XGBClassifier()
        model.load_model(model_path)
    else:
        with open(model_path, "r") as f:
            data = json.load(f)
        model = LogisticRegression()
        model.coef_ = np.array(data["coef"])
        model.intercept_ = np.array(data["intercept"])
        model.classes_ = np.array([0, 1])
    return model

def save_label_encoders(label_encoders):
    with open(LABEL_ENCODERS_PATH, "w") as f:
        json.dump({col: le.classes_.tolist() for col, le in label_encoders.items()}, f)

def load_label_encoders():
    with open(LABEL_ENCODERS_PATH, "r") as f:
        return {col: LabelEncoder().fit(classes) for col, classes in json.load(f).items()}

def save_scaler(scaler):
    with open(SCALER_PATH, "w") as f:
        json.dump({"mean": scaler.mean_.tolist(), "scale": scaler.scale_.tolist()}, f)

def load_scaler():
    with open(SCALER_PATH, "r") as f:
        data = json.load(f)
    scaler = StandardScaler()
    scaler.mean_ = np.array(data["mean"])
    scaler.scale_ = np.array(data["scale"])
    return scaler

def load_feature_names():
    with open(FEATURES_PATH, "r") as f:
        return json.load(f)

def encode_categorical(df):
    label_encoders = {}
    for col in df.select_dtypes(include=['object']).columns:
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col].astype(str))
        label_encoders[col] = le
    return df, label_encoders

def align_features(df, feature_names):
    for col in feature_names:
        if col not in df.columns:
            df[col] = 0
    return df[feature_names]

def aggregate_feature_group(df, prefix, agg_funcs=["sum", "mean", "max"]):
    group_cols = [col for col in df.columns if col.startswith(prefix)]
    if not group_cols:
        return df
    for func in agg_funcs:
        new_col = f"{prefix}{func}"
        if func == "sum":
            df[new_col] = df[group_cols].sum(axis=1)
        elif func == "mean":
            df[new_col] = df[group_cols].mean(axis=1)
        elif func == "max":
            df[new_col] = df[group_cols].max(axis=1)
    return df



# –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å
st.title("üöÄ –ê–Ω—Ç–∏—Ñ—Ä–æ–¥ ML")

if "–û–±—É—á–µ–Ω–∏–µ" in menu:
    st.header(f"üìå –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ {model_type}")
    uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ CSV –∏–ª–∏ Excel-—Ñ–∞–π–ª", type=["csv", "xlsx", "xls"])
    if uploaded_file:
        df = pd.read_csv(uploaded_file) if uploaded_file.name.endswith(".csv") else pd.read_excel(uploaded_file)
        st.dataframe(df.head())
        if "GB_flag" not in df.columns:
            st.error("‚ùå –û—à–∏–±–∫–∞: –í —Ñ–∞–π–ª–µ –Ω–µ—Ç —Å—Ç–æ–ª–±—Ü–∞ 'GB_flag'")
        else:
            X, y = df.drop(columns=["GB_flag"]), df["GB_flag"]
            X_raw = X.copy()
            X = aggregate_feature_group(X, "MONTH_OVERDUE_")
            X_raw = aggregate_feature_group(X_raw, "MONTH_OVERDUE_")

            X, label_encoders = encode_categorical(X)
            imputer = SimpleImputer(strategy="mean")
            X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            X_original_for_export = X.copy()
            X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)
            _, X_test_raw = train_test_split(X_raw, test_size=0.3, random_state=42)
            feature_names = list(X.columns)
            model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42, eval_metric="logloss", n_jobs=-1) if model_type == "XGBoost" else LogisticRegression()
            model.fit(X_train, y_train)
            # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –∑–∞–º–µ—Ä–æ–º –≤—Ä–µ–º–µ–Ω–∏
            start_time = time.time()
            model.fit(X_train, y_train)
            train_time = time.time() - start_time

            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            y_train_pred = model.predict(X_train)
            y_test_pred = model.predict(X_test)
            y_test_proba = model.predict_proba(X_test)[:, 1]

            if model_type == "XGBoost":
                st.subheader("üß† SHAP: –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –º–æ–¥–µ–ª–∏")
                explainer = shap.Explainer(model, X_train)
                shap_values = explainer(X_test)

                st.subheader("üìå SHAP Summary Plot")
                fig_summary = plt.figure()
                shap.plots.beeswarm(shap_values, show=False)
                st.pyplot(fig_summary)

                st.subheader("üîç SHAP Force Plot (–ø—Ä–∏–º–µ—Ä —Å matplotlib)")

                index_to_explain = st.slider("–í—ã–±–µ—Ä–∏—Ç–µ –∏–Ω–¥–µ–∫—Å –ø—Ä–∏–º–µ—Ä–∞ –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏", 0, len(shap_values) - 1, 0)

                fig = plt.figure()
                shap.plots.force(
                    explainer.expected_value,
                    shap_values[index_to_explain].values,
                    matplotlib=True
                )
                st.pyplot(fig)

            # –ú–µ—Ç—Ä–∏–∫–∏
            accuracy = accuracy_score(y_test, y_test_pred)
            precision = precision_score(y_test, y_test_pred)
            recall = recall_score(y_test, y_test_pred)
            roc_auc = roc_auc_score(y_test, y_test_proba)
            gini = 2 * roc_auc - 1


            # –ò–Ω–¥–µ–∫—Å –ö–æ–ª–º–æ–≥–æ—Ä–æ–≤–∞-–°–º–∏—Ä–Ω–æ–≤–∞
            def ks_statistic(y_true, y_proba):
                from scipy.stats import ks_2samp
                return ks_2samp(y_proba[y_true == 1], y_proba[y_true == 0]).statistic


            ks = ks_statistic(y_test.to_numpy(), y_test_proba)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å—ë
            model_path = MODEL_PATH_XGB if model_type == "XGBoost" else MODEL_PATH_LR
            save_model(model, feature_names, model_path)
            save_label_encoders(label_encoders)
            save_scaler(scaler)

            # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            st.success(f"‚úÖ {model_type} –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")

            st.subheader("üìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è")
            st.markdown(f"‚è± **–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è:** {train_time:.2f} —Å–µ–∫")
            st.markdown(f"üìä **Gini –∏–Ω–¥–µ–∫—Å:** {gini:.4f}")
            st.markdown(f"üìä **KS –∏–Ω–¥–µ–∫—Å:** {ks:.4f}")
            st.markdown(f"‚úÖ **Accuracy:** {accuracy:.4f}")
            st.markdown(f"üéØ **Precision:** {precision:.4f}")
            st.markdown(f"üîÅ **Recall:** {recall:.4f}")
            # SHAP –∞–Ω–∞–ª–∏–∑
            st.subheader("üß† SHAP: –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –º–æ–¥–µ–ª–∏")

            if model_type == "XGBoost":
                explainer = shap.Explainer(model, X_train)
                shap_values = explainer(X_test)

                # Summary plot (–æ–±—â–µ–µ –≤–ª–∏—è–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
                st.subheader("üìå SHAP Summary Plot")
                fig_summary = plt.figure()
                shap.plots.beeswarm(shap_values, show=False)
                st.pyplot(fig_summary)

                # Force plot (–æ–¥–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ)
                st.subheader("üîç SHAP Force Plot (–ø—Ä–∏–º–µ—Ä —Å matplotlib)")

                index_to_explain = st.slider("–í—ã–±–µ—Ä–∏—Ç–µ –∏–Ω–¥–µ–∫—Å –ø—Ä–∏–º–µ—Ä–∞ –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏", 0, len(shap_values) - 1, 0)

                fig = plt.figure()
                shap.plots.force(
                    explainer.expected_value,
                    shap_values[index_to_explain].values,
                    matplotlib=True
                )
                st.pyplot(fig)

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            st.subheader("üî¢ –ü—Ä–∏–º–µ—Ä —Ä–∞—Å—á—ë—Ç–æ–≤")
            # –¢–∞–±–ª–∏—Ü–∞ –º–µ—Ç—Ä–∏–∫
            metrics_df = pd.DataFrame({
                "–ú–µ—Ç—Ä–∏–∫–∞": ["–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è (—Å–µ–∫)", "Gini", "KS", "Accuracy", "Precision", "Recall", "ROC AUC"],
                "–ó–Ω–∞—á–µ–Ω–∏–µ": [round(train_time, 2), round(gini, 4), round(ks, 4), round(accuracy, 4),
                             round(precision, 4), round(recall, 4), round(roc_auc, 4)]
            })
            st.subheader("üìä –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –º–µ—Ç—Ä–∏–∫")
            st.table(metrics_df)

            # ROC-–∫—Ä–∏–≤–∞—è
            fpr, tpr, _ = roc_curve(y_test, y_test_proba)
            fig_roc = plt.figure()
            plt.plot(fpr, tpr, label=f"ROC AUC = {roc_auc:.2f}")
            plt.plot([0, 1], [0, 1], linestyle="--", color="gray")
            plt.xlabel("False Positive Rate")
            plt.ylabel("True Positive Rate")
            plt.title("ROC-–∫—Ä–∏–≤–∞—è")
            plt.legend()
            st.subheader("üìà ROC-–∫—Ä–∏–≤–∞—è")
            st.pyplot(fig_roc)

            # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
            fig_hist = plt.figure()
            plt.hist(y_test_proba[y_test == 0], bins=30, alpha=0.6, label="–ö–ª–∞—Å—Å 0 (–Ω–µ –º–æ—à–µ–Ω–Ω–∏–∫)")
            plt.hist(y_test_proba[y_test == 1], bins=30, alpha=0.6, label="–ö–ª–∞—Å—Å 1 (–º–æ—à–µ–Ω–Ω–∏–∫)")
            plt.title("üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –ø–æ –∫–ª–∞—Å—Å–∞–º")
            plt.xlabel("–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å")
            plt.ylabel("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ")
            plt.legend()
            st.subheader("üîç –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π")
            st.pyplot(fig_hist)
            st.markdown(f"**–†–∞—Å—Å—á–∏—Ç–∞–Ω–Ω—ã–π –±–∞–ª–ª (–ø—Ä–∏–º–µ—Ä):** {y_test_pred[0]}")
            st.markdown(f"**–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ (–ø—Ä–∏–º–µ—Ä):** {y_test_proba[0]:.4f}")

            # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
            st.subheader("üìå –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ")
            st.code(", ".join(feature_names))

            # –¢–∞—Ä–≥–µ—Ç –∏ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç—å –∫ –≤—ã–±–æ—Ä–∫–µ
            subset_info = pd.DataFrame({
                "target": pd.concat([y_train, y_test], ignore_index=True),
                "subset": ["train"] * len(y_train) + ["test"] * len(y_test)
            })
            st.subheader("üß™ –†–∞–∑–º–µ—Ç–∫–∞ –≤—ã–±–æ—Ä–æ–∫")
            st.dataframe(subset_info.sample(10, random_state=42))
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ DataFrame
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º X_test –¥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è
            X_test_original = X_test_raw.copy()


            # –§–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º —Å –Ω—É–∂–Ω–æ–π –∏–Ω—Ñ–æ–π
            # –°–æ–∑–¥–∞—ë–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π DataFrame –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            results_df = X_test_raw.copy()
            results_df["–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å"] = y_test_pred
            results_df["–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å"] = y_test_proba
            results_df["–ò—Å—Ç–∏–Ω–Ω—ã–π GB_flag"] = y_test.values

            # –ö–Ω–æ–ø–∫–∞ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
            csv = results_df.to_csv(index=False).encode("utf-8")
            st.download_button(
                label="üì• –°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è)",
                data=csv,
                file_name="test_predictions.csv",
                mime="text/csv"
            )

            st.success(f"‚úÖ {model_type} –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")

elif "–ü—Ä–æ–≤–µ—Ä–∫–∞" in menu:
    model_path = MODEL_PATH_XGB if model_type == "xgb" else MODEL_PATH_LR
    if os.path.exists(model_path):
        model = load_model(model_path, model_type)
        scaler = load_scaler()
        feature_names = load_feature_names()
        uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ç–µ—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª", type=["csv", "xlsx", "xls"])

        if uploaded_file:
            df = pd.read_csv(uploaded_file) if uploaded_file.name.endswith(".csv") else pd.read_excel(uploaded_file)
            df_original = df.copy()

            # –í—ã–±–æ—Ä –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é —Ñ–ª–∞–∂–∫–æ–≤
            st.sidebar.subheader("üîß –í—ã–±–æ—Ä –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö")
            selected_features = st.sidebar.multiselect("–û—Ç–º–µ—Ç—å—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è", df.columns.tolist(),
                                                       default=df.columns.tolist())

            df_aligned = align_features(df, feature_names)
            imputer = SimpleImputer(strategy="mean")
            df_imputed = pd.DataFrame(imputer.fit_transform(df_aligned), columns=df_aligned.columns)
            df_scaled = pd.DataFrame(scaler.transform(df_imputed), columns=df_imputed.columns)

            predictions = model.predict_proba(df_scaled)[:, 1]
            df_original["–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞"] = predictions
            if model_type == "xgb":
                st.subheader("üß† SHAP: –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π")
                explainer = shap.Explainer(model, df_scaled)
                shap_values = explainer(df_scaled)



                st.subheader("üìå SHAP Summary Plot")
                fig_check = plt.figure()
                shap.plots.beeswarm(shap_values, show=False)
                st.pyplot(fig_check)

                st.subheader("üîç SHAP Force Plot (–ø—Ä–∏–º–µ—Ä —Å matplotlib)")

                index_to_explain = st.slider("–í—ã–±–µ—Ä–∏—Ç–µ –∏–Ω–¥–µ–∫—Å –ø—Ä–∏–º–µ—Ä–∞ –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏", 0, len(shap_values) - 1, 0)

                fig = plt.figure()
                shap.plots.force(
                    explainer.expected_value,
                    shap_values[index_to_explain].values,
                    matplotlib=True
                )
                st.pyplot(fig)

            # –ü–æ–ª–∑—É–Ω–æ–∫ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–æ–∫
            st.subheader("üìã –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–∫–∏")
            rows_to_display = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–æ–±—Ä–∞–∂–∞–µ–º—ã—Ö —Å—Ç—Ä–æ–∫", min_value=1,
                                        max_value=min(100, len(df_original)), value=min(10, len(df_original)), step=1)

            # –ò—Ç–æ–≥–æ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞
            st.dataframe(df_original[selected_features + ["–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞"]].head(rows_to_display))
    else:
        st.error("‚ùå –°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å")
